# -*- coding: utf-8 -*-
"""model_preparation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Cv1pGVt5TDnJatb-T_DuNcgvQkupvTnq
"""

!pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html
!pip install transformers requests beautifulsoup4 pandas numpy

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import requests
from bs4 import BeautifulSoup
import re

tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')

model1 = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')

tokens = tokenizer.encode('ver bad quality', return_tensors='pt')

tokens #encoded to list of numbers

#just to see what decode function do

result = model1(tokens)
result

result.logits

result.logits[0]

int(torch.argmax(result.logits))+1

import pickle
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Define the model and tokenizer
tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')
model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')

# Create a dictionary to store both the tokenizer and the model
model_and_tokenizer = {
    'tokenizer': tokenizer,
    'model': model
}

# Save the dictionary to a pickle file
with open('model_and_tokenizer.pkl', 'wb') as file:
    pickle.dump(model_and_tokenizer, file)

r = requests.get('https://www.yelp.com/biz/social-brew-cafe-pyrmont')
soup = BeautifulSoup(r.text, 'html.parser')
regex = re.compile('.*comment.*')
results = soup.find_all('p', {'class':regex})
reviews = [result.text for result in results]

# https://www.yelp.com/biz/social-brew-cafe-pyrmont

reviews

import numpy as np
import pandas as pd

df = pd.DataFrame(np.array(reviews), columns=['review'])
df

def sentiment_score(review):
    tokens = tokenizer.encode(review, return_tensors='pt')
    result = model1(tokens)
    return int(torch.argmax(result.logits))+1

sentiment_score(df['review'].iloc[1])

print(len(df.index))

#  for i in range(0,512):
#     df['score'].iloc[i]=sentiment_score(df['review'].iloc[i])
df['sentiment'] = df['review'].apply(lambda x: sentiment_score(x[:512]))

df

df.tail()

"""**Emotion** **Detection**"""

import tensorflow as tf

import keras

import pandas as pd
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
import re
train=pd.read_table('train.txt', delimiter = ';', header=None, )
val=pd.read_table('val.txt', delimiter = ';', header=None, )
test=pd.read_table('test.txt', delimiter = ';', header=None, )

data = pd.concat([train ,  val , test])
data.columns = ["text", "label"]

data.shape

data.isna().any(axis=1).sum()

#text preprocessing
ps = PorterStemmer()

def preprocess(line):
    review = re.sub('[^a-zA-Z]', ' ', line) #leave only characters from a to z
    review = review.lower() #lower the text
    review = review.split() #turn string into list of words
    #apply Stemming
    review = [ps.stem(word) for word in review if not word in stopwords.words('english')] #delete stop words like I, and ,OR   review = ' '.join(review)
    #trun list into sentences
    return " ".join(review)

import nltk
nltk.download('stopwords')

data['text']=data['text'].apply(lambda x: preprocess(x))

from sklearn import preprocessing

label_encoder = preprocessing.LabelEncoder()
data['N_label'] = label_encoder.fit_transform(data['label'])

data['text']

from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer(max_features=5000,ngram_range=(1,3))#example: the course was long-> [the,the course,the course was,course, course was, course was long,...]

data_cv = cv.fit_transform(data['text']).toarray()

data_cv

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test =train_test_split(data_cv, data['N_label'], test_size=0.25, random_state=42)

# first neural network with keras tutorial
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
# load the dataset
# split into input (X) and output (y) variables
# define the keras model
model = Sequential()
model.add(Dense(12, input_shape=(X_train.shape[1],), activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(6, activation='softmax'))
# compile the keras model
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
# fit the keras model on the dataset
model.fit(X_train, y_train, epochs=10, batch_size=10)
# evaluate the keras model
_, accuracy = model.evaluate(X_train, y_train)
print('Accuracy: %.2f' % (accuracy*100))

_, accuracy = model.evaluate(X_test, y_test)
print('Accuracy: %.2f' % (accuracy*100))

import pandas as pd
import numpy as np

text='Some of the best Milkshakes me and my daughter ever tasted. MMMMMM HMMMMMMMM.'
text=preprocess(text)
array = cv.transform([text]).toarray()
pred = model.predict(array)
a=np.argmax(pred, axis=1)
label_encoder.inverse_transform(a)[0]

tf.keras.models.save_model(model,'my_model.keras')

import pickle
vectorizer_filename = 'count_vectorizer.pkl'
with open(vectorizer_filename, 'wb') as file:
    pickle.dump(cv, file)

label_encoder_filename='label_encoder.pkl'
with open(label_encoder_filename,'wb')as file:
  pickle.dump(label_encoder,file)

import tensorflow as tf

# Load the model from the saved HDF5 file
loaded_model = tf.keras.models.load_model('my_model.keras')
model_and_weights = {
    'model': loaded_model
}
import pickle

# Save the dictionary with the model to a pickle file
with open('model_pickle.pkl', 'wb') as file:
    pickle.dump(model_and_weights, file)

df['review'].iloc[1]

def predict_emotion(text, model, cv, label_encoder):
    # Preprocess the input text
    text = preprocess(text)

    # Transform the preprocessed text into a feature array
    array = cv.transform([text]).toarray()

    # Make predictions using the model
    pred = model.predict(array)

    # Get the predicted class
    predicted_class = np.argmax(pred, axis=1)[0]

    # Inverse transform the predicted class to get the emotion label
    predicted_emotion = label_encoder.inverse_transform([predicted_class])[0]

    return predicted_emotion

text='you are a beautiful girl.'
# text=preprocess(text)
# array = cv.transform([text]).toarray()
import pickle

# Load the model from the pickle file
with open('model_pickle.pkl', 'rb') as file:
    loaded_model_dict = pickle.load(file)

loaded_model = loaded_model_dict['model']

# Use the loaded model for inference
predictions = loaded_model.predict(array)

a=np.argmax(predictions, axis=1)
label_encoder.inverse_transform(a)[0]

# for i in range(0,len(df)):
#   df['emotion'].iloc[i]=emotion_f(df['review'].iloc[i])

  # for i in range(0,512):
#     df['score'].iloc[i]=sentiment_score(df['review'].iloc[i])

print(predict_emotion(df['review'].iloc[0],model,cv,label_encoder))

df['emotion_score'] = df['review'].apply(lambda x: predict_emotion(x[:512],model,cv,label_encoder))

df

df3=df.copy()

df3

# df = pd.DataFrame(data)

# Specify the filename for the Excel file
excel_filename = 'data2.xlsx'

# Save the DataFrame to an Excel file
df.to_excel(excel_filename, index=False)

df

# df = df.rename(columns={'label': 'emotion', 'N_label': 'sentiment_score','text':'reviews'})

df

def catogery(review):
  for i in range(0,len(df)):
    if df['sentiment']>3 and (df['emotion_score']=="joy" or df['emotion_score']=="love" or df['emotion_score']=="surprise"):
      df['category']="good"
    elif df['sentiment']<=3 and (df['emotion_score']=="sad" or df['emotion_score']=="sad" ):
      df['category']="bad"

df

df2 = df.copy()

df2



import pandas as pd

def categorize_reviews(df):
    for index, row in df.iterrows():
        sentiment = row['sentiment']
        emotion_score = row['emotion_score']

        if sentiment > 3 and (emotion_score == "joy" or emotion_score == "love" or emotion_score == "surprise"):
            df.at[index, 'category'] = 'good'
        elif sentiment <= 3 and (emotion_score == "sadness" or emotion_score == "anger"):
            df.at[index, 'category'] = 'bad'
        elif sentiment <= 3 and (emotion_score == "joy" or emotion_score == "love" or emotion_score == "surprise"):
            df.at[index, 'category'] = 'good'
        elif sentiment > 3 and (emotion_score == "sadness" or emotion_score == "anger"):
            df.at[index, 'category'] = 'bad'
    return df

# # Example usage:
# data = {
#     'sentiment': [4, 2, 5, 3, 1],
#     'emotion_score': ['joy', 'anger', 'love', 'sadness', 'surprise']
# }
# df = pd.DataFrame(data)

df2 = categorize_reviews(df2)

# print(df)

df

df2

df2

# Specify the filename for the Excel file
excel_filename = 'categorised_data2.xlsx'

# Save the DataFrame to an Excel file
df2.to_excel(excel_filename, index=False)

excel_filename = 'categorised_data2.xlsx'
df2 = pd.read_excel(excel_filename)
df2

# text=input("enter your comment")
# print(sentiment_score(text))
# print(predict_emotion(text,model,cv,label_encoder))

# !pip install streamlit

# import streamlit as st
# import pandas as pd


# def main():
# 	st.title("Emotion Detector")
# 	menu = ["Home","About"]
# 	choice = st.sidebar.selectbox("Menu",menu)

# 	if choice == "Home":
# 		st.subheader("Enter a text here")
# 		with st.form(key='form1'):
# 			inputText = st.text_input("Type here")
# 			submit_button = st.form_submit_button(label='Submit')

# 		if submit_button:
# 			st.write("Input is : {}".format(inputText))
# 			# write code here to work on input text
#             #
#             #
#             #
#             #
#             #
#             #

# if __name__ == '__main__':
# 	main()

!streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py

